import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
from itertools import product

# âœ… ë°ì´í„° ë¡œë“œ
df = pd.read_csv("augmented_sensor_data.csv")
emotion_mapping = dict(zip(df["emotion_label"].unique(), df["emotion"].unique()))  # {ë¼ë²¨ ë²ˆí˜¸: ê°ì •}
print(f"ğŸ“¢ ê°ì • ë¼ë²¨ ë§¤í•‘ í™•ì¸: {emotion_mapping}")

# âœ… ê°ì •ë³„ ë°ì´í„° ê°œìˆ˜ ì¶œë ¥
label_counts = df["emotion_label"].value_counts().sort_index()
#ì¸ë±ìŠ¤ ë²ˆí˜¸ ìˆœì„œ-sort_index()
print("ğŸ“Š ê°ì • ë¼ë²¨ ê°œìˆ˜ í™•ì¸:")
for label, count in label_counts.items():
    print(f"    {emotion_mapping[label]} ({label}): {count}ê°œ")

# âœ… ì…ë ¥ê°’ (X)ì™€ ì •ë‹µê°’ (y)
X = df[["heart_rate", "gsr", "gsr_diff"]].values
y = df["emotion_label"].values

# âœ… ë°ì´í„° ì •ê·œí™” (í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
mean = X.mean(axis=0)
scale = X.std(axis=0)
X = (X - mean) / scale  # ì •ê·œí™” ì ìš©

# âœ… ì›-í•« ì¸ì½”ë”© ì ìš© (ë‹¤ì¤‘ ë¶„ë¥˜)
num_classes = len(np.unique(y))
#pdëŠ” ë°œê²¬ëœ ìˆœì„œëŒ€ë¡œ, npëŠ” ì˜¤ë¦„ì°¨ìˆœ
y_one_hot = np.zeros((len(y), num_classes))
#ë™ì¼í•œ í¬ê¸°ì˜ ì œë¡œê³µê°„ ìƒì„±
y_one_hot[np.arange(len(y)), y] = 1
#ì •ë‹µ ê³µê°„ì— 1 ë„£ê¸°

# âœ… ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ (ì•ˆì •ì„± ê°œì„ )
def softmax(x):
    x = x - np.max(x, axis=1, keepdims=True)
    #ì œì¼ í° ê°’ ë¹¼ê¸°
    exp_x = np.exp(x)
    #ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì ìš©
    return exp_x / exp_x.sum(axis=1, keepdims=True)
    #ëª¨ë“  í•©ì´ 1ë¡œ ë˜ë„ë¡ í•¨

# âœ… ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# âœ… ìˆœì „íŒŒ (Forward Propagation)
def forward(X, W1, W2):
    X_bias = np.column_stack((X, np.ones(len(X))))#(1,3+1)
    Z1 = np.dot(X_bias, W1)#(1,3+1),(3+1,h) -- (1,h)
    A1 = sigmoid(Z1)#(1,h)  

    H_bias = np.column_stack((A1, np.ones(len(A1))))#(1,h+1)  
    Z2 = np.dot(H_bias, W2)#(1,h+1),(h+1,4)
    A2 = softmax(Z2)#(1,4)  
    return A2, Z1, A1, Z2, X_bias, H_bias

# âœ… ì—­ì „íŒŒ (Backpropagation)
def backward(X, X_bias, H_bias, y, A2, Z1, A1, W1, W2, lr):
    out_err = A2 - y#(1,4)-y
    out_delta = out_err  

    hid_err = out_delta.dot(W2[:-1, :].T)#(1,4)*(4,h) -> (1,h)
    hid_delta = hid_err * (A1 * (1 - A1))#(1,h) xê³±x ((1,h) x 1-(1,h))

    #ê²½ì‚¬í•˜ê°•ë²•
    W2 -= H_bias.T.dot(out_delta) * lr  #(h+1,1),(1,4) -> (h+1,4)
    W1 -= X_bias.T.dot(hid_delta) * lr  #(3+1,1),(1,h) -> (3+1,h)

    return W1, W2

# âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
hidden_layer_sizes = range(4, 10)
learning_rates = [0.01, 0.005, 0.001]
epochs = 5000  

best_accuracy = 0
best_params = {}

print("\nğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì‹œì‘...\n")
results = []

for hidden_size, learning_rate in product(hidden_layer_sizes, learning_rates):
    print(f"\nğŸš€ í…ŒìŠ¤íŠ¸ ì¤‘ - ì€ë‹‰ì¸µ: {hidden_size}, í•™ìŠµë¥ : {learning_rate}")

    # âœ… ëª¨ë¸ ì´ˆê¸°í™”
    input_size = X.shape[1]
    #.shape[1]-ì—´ì˜ ê°œìˆ˜ (3ê°œ)
    output_size = num_classes
    #0,1,2,3 - 4ê°œ
  
    #ì´ˆê¸° ê°€ì¤‘ì¹˜ê°’
    W1 = np.random.randn(input_size + 1, hidden_size) * 0.01
    W2 = np.random.randn(hidden_size + 1, output_size) * 0.01

    mse_list = []
    acc_list = []

    for epoch in range(epochs):
        A2, Z1, A1, Z2, X_bias, H_bias = forward(X, W1, W2)
        W1, W2 = backward(X, X_bias, H_bias, y_one_hot, A2, Z1, A1, W1, W2, learning_rate)

        mse = np.mean((y_one_hot - A2) ** 2)
        mse_list.append(mse)

        predictions = np.argmax(A2, axis=1)#(1,4)
        accuracy = np.mean(predictions == y)
        acc_list.append(accuracy)

        if epoch % 500 == 0 or epoch == epochs - 1:
            print(f"ğŸ“¢ Epoch {epoch}/{epochs} - MSE: {mse:.4f}, Accuracy: {accuracy:.4f}")

    results.append({
        "hidden_size": hidden_size,
        "learning_rate": learning_rate,
        "epochs": epochs,
        "accuracy": accuracy
    })

    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_params = {
            "hidden_size": hidden_size,
            "learning_rate": learning_rate,
            "epochs": epochs,
            "accuracy": accuracy
        }
        # âœ… ëª¨ë¸ ì €ì¥ (ì •ê·œí™” ì •ë³´ë„ í•¨ê»˜ ì €ì¥)
        with open("best_ann_model.pkl", "wb") as f:
            pickle.dump((W1, W2, emotion_mapping, mean, scale), f)

print("\nğŸ¯ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì™„ë£Œ!\n")
print(f"âœ… ì€ë‹‰ì¸µ ê°œìˆ˜: {best_params['hidden_size']}")
print(f"âœ… í•™ìŠµë¥ : {best_params['learning_rate']}")
print(f"âœ… ì—í¬í¬: {best_params['epochs']}")
print(f"âœ… ìµœì¢… ì •í™•ë„: {best_params['accuracy']:.4f}")

# âœ… ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
with open("best_hyperparameters.pkl", "wb") as f:
    pickle.dump(best_params, f)
