import numpy as np
import pandas as pd
import pickle
from itertools import product

# âœ… ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# âœ… ìˆœì „íŒŒ (Forward Propagation)
def forward(X, W1, W2):
    X_bias = np.column_stack((X, np.ones(len(X))))
    Z1 = np.dot(X_bias, W1)
    A1 = sigmoid(Z1)

    H_bias = np.column_stack((A1, np.ones(len(A1))))
    Z2 = np.dot(H_bias, W2)
    A2 = sigmoid(Z2)
    return A2, Z1, A1, Z2, X_bias, H_bias

# âœ… ì—­ì „íŒŒ (Backpropagation)
def backward(X, X_bias, H_bias, y, A2, Z1, A1, W1, W2, lr):
    out_err = y - A2
    out_delta = out_err * sigmoid_derivative(A2)

    hid_err = out_delta.dot(W2[:-1, :].T)
    hid_delta = hid_err * sigmoid_derivative(A1)

    W2 += H_bias.T.dot(out_delta) * lr
    W1 += X_bias.T.dot(hid_delta) * lr

    return W1, W2

# âœ… ë°ì´í„° ë¡œë“œ
df = pd.read_csv("augmented_sensor_data.csv")
df = df[(df["heart_rate"] != -1) & (df["gsr"] != -1)]

# âœ… ê°ì • ë¼ë²¨ ê°œìˆ˜ í™•ì¸ (ê¸°íƒ€(Other) ì œì™¸)
emotion_counts = df[df["emotion_label"] != 1]["emotion_label"].value_counts().sort_index()
print("ğŸ“¢ ê°ì • ë¼ë²¨ ê°œìˆ˜ í™•ì¸ (ê¸°íƒ€ ì œì™¸):\n", emotion_counts)

# âœ… ê°€ì¥ ì ì€ ê°œìˆ˜ë¥¼ ê°€ì§„ ê°ì • ì°¾ê¸° & ê°€ì¤‘ì¹˜ ì„¤ì •
min_label = emotion_counts.idxmin()  # ê°€ì¥ ì ì€ ê°œìˆ˜ë¥¼ ê°€ì§„ ê°ì • ë¼ë²¨
weight_label = min_label  # í•´ë‹¹ ê°ì •ì— ê°€ì¤‘ì¹˜ ì ìš©
weight_value = 20  # ê°€ì¤‘ì¹˜ 1.5ë°° ì ìš©

print(f"âœ… ê°€ì¥ ì ì€ ê°œìˆ˜ë¥¼ ê°€ì§„ ê°ì •: {weight_label}, ê°€ì¤‘ì¹˜ {weight_value} ì ìš©ë¨.")

# âœ… ì…ë ¥ê°’ (X)ì™€ ì •ë‹µê°’ (y)
X = df[["heart_rate", "gsr", "gsr_diff"]].values
y = df["emotion_label"].values.reshape(-1, 1)

# âœ… ë°ì´í„° ì •ê·œí™” (í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
X = (X - X.mean(axis=0)) / X.std(axis=0)

# âœ… ì‹¤í—˜í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì • (ì—í¬í¬ëŠ” 10,000ìœ¼ë¡œ ê³ ì •)
hidden_layer_sizes = range(1, 15)  # ì€ë‹‰ì¸µ ë‰´ëŸ° ê°œìˆ˜ (1 ~ 15)
learning_rates = [0.1, 0.05, 0.01, 0.005, 0.001]  # í•™ìŠµë¥  ë²”ìœ„
epochs = 10000  # ì—í¬í¬ëŠ” 10,000ìœ¼ë¡œ ê³ ì •

# âœ… ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ ë³€ìˆ˜
best_accuracy = 0
best_params = {}

# âœ… ëª¨ë“  ì¡°í•©ì„ ì‹œë„í•˜ì—¬ ìµœì ê°’ ì°¾ê¸°
for hidden_size, learning_rate in product(hidden_layer_sizes, learning_rates):
    print(f"\nğŸš€ í…ŒìŠ¤íŠ¸ ì¤‘ - ì€ë‹‰ì¸µ: {hidden_size}, í•™ìŠµë¥ : {learning_rate}, ì—í¬í¬: {epochs}")

    # âœ… ëª¨ë¸ ì´ˆê¸°í™”
    input_size = X.shape[1]
    output_size = 1
    W1 = np.random.randn(input_size + 1, hidden_size) * 0.01
    W2 = np.random.randn(hidden_size + 1, output_size) * 0.01

    # âœ… í•™ìŠµ ì‹¤í–‰
    for epoch in range(epochs):
        A2, Z1, A1, Z2, X_bias, H_bias = forward(X, W1, W2)
        W1, W2 = backward(X, X_bias, H_bias, y, A2, Z1, A1, W1, W2, learning_rate)

        # âœ… MSE (Mean Squared Error) ê³„ì‚°
        mse = np.mean((y - A2) ** 2)

        # âœ… Accuracy (ì •í™•ë„) ê³„ì‚°
        predictions = np.round(A2)
        accuracy = np.mean(predictions == y)

        if epoch % 2000 == 0:
            print(f"ğŸ“¢ Epoch {epoch}/{epochs} - MSE: {mse:.4f}, Accuracy: {accuracy:.4f}")

    # âœ… ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (ê°€ì¥ ë†’ì€ ì •í™•ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ)
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_params = {
            "hidden_size": hidden_size,
            "learning_rate": learning_rate,
            "epochs": epochs,
            "accuracy": accuracy
        }
        # âœ… ìµœì ì˜ ëª¨ë¸ ì €ì¥
        with open("best_ann_model.pkl", "wb") as f:
            pickle.dump((W1, W2), f)

print("\nğŸ¯ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ìŒ!")
print(f"âœ… ì€ë‹‰ì¸µ: {best_params['hidden_size']}, í•™ìŠµë¥ : {best_params['learning_rate']}, ì—í¬í¬: {best_params['epochs']}")
print(f"âœ… ìµœì¢… Accuracy: {best_params['accuracy']:.4f}")

# âœ… ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
with open("best_hyperparameters.pkl", "wb") as f:
    pickle.dump(best_params, f)
