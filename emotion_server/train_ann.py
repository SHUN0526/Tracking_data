import json
import pandas as pd
import numpy as np
import pickle
from sklearn.preprocessing import LabelEncoder
from itertools import product

# âœ… ì‚¬ìš©ì JSON íŒŒì¼ ë¡œë“œ
with open("emotion_labeling_input.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# âœ… JSON â†’ DataFrame ë³€í™˜
sensor_records = []
for item in data["emotion_labeling_data"]:
    emotion = item["label"]
    for record in item["labeled_data"]:
        record["emotion"] = emotion
        sensor_records.append(record)

df = pd.DataFrame(sensor_records)

# âœ… ì „ì²˜ë¦¬: ê²°ì¸¡ê°’ ë° -1 ì œê±°
df.replace(-1, np.nan, inplace=True)
df.dropna(inplace=True)

# âœ… GSR ë³€í™”ëŸ‰ ê³„ì‚°
df["gsr_diff"] = df["gsr"].diff().fillna(0)

# âœ… ê°ì • ë¼ë²¨ ì¸ì½”ë”©
label_encoder = LabelEncoder()
df["emotion_label"] = label_encoder.fit_transform(df["emotion"])
emotion_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))
print("ğŸ“Š ê°ì • ë¼ë²¨ ë§¤í•‘:", emotion_mapping)

# âœ… í•™ìŠµ ì…ë ¥ê°’/ì •ë‹µê°’ ì„¤ì •
X = df[["heart_rate", "gsr", "gsr_diff"]].values
y = df["emotion_label"].values

# âœ… ì…ë ¥ ì •ê·œí™”
mean = X.mean(axis=0)
scale = X.std(axis=0)
X = (X - mean) / scale

# âœ… ì›-í•« ì¸ì½”ë”©
num_classes = len(np.unique(y))
y_one_hot = np.zeros((len(y), num_classes))
y_one_hot[np.arange(len(y)), y] = 1

# âœ… ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ (ì•ˆì •ì„± ê°œì„ )
def softmax(x):
    x = x - np.max(x, axis=1, keepdims=True)
    #ì œì¼ í° ê°’ ë¹¼ê¸°
    exp_x = np.exp(x)
    #ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì ìš©
    return exp_x / exp_x.sum(axis=1, keepdims=True)
    #ëª¨ë“  í•©ì´ 1ë¡œ ë˜ë„ë¡ í•¨

# âœ… ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# âœ… ìˆœì „íŒŒ (Forward Propagation)
def forward(X, W1, W2):
    X_bias = np.column_stack((X, np.ones(len(X))))#(1,3+1)
    Z1 = np.dot(X_bias, W1)#(1,3+1),(3+1,h) -- (1,h)
    A1 = sigmoid(Z1)#(1,h)  

    H_bias = np.column_stack((A1, np.ones(len(A1))))#(1,h+1)  
    Z2 = np.dot(H_bias, W2)#(1,h+1),(h+1,4)
    A2 = softmax(Z2)#(1,4)  
    return A2, Z1, A1, Z2, X_bias, H_bias

# âœ… ì—­ì „íŒŒ (Backpropagation)
def backward(X, X_bias, H_bias, y, A2, Z1, A1, W1, W2, lr):
    out_err = A2 - y#(1,4)-y
    out_delta = out_err  

    hid_err = out_delta.dot(W2[:-1, :].T)#(1,4)*(4,h) -> (1,h)
    hid_delta = hid_err * (A1 * (1 - A1))#(1,h) xê³±x ((1,h) x 1-(1,h))

    #ê²½ì‚¬í•˜ê°•ë²•
    W2 -= H_bias.T.dot(out_delta) * lr  #(h+1,1),(1,4) -> (h+1,4)
    W1 -= X_bias.T.dot(hid_delta) * lr  #(3+1,1),(1,h) -> (3+1,h)

    return W1, W2

# âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
hidden_layer_sizes = range(4, 10)
learning_rates = [0.01, 0.005, 0.001]
epochs = 5000  

best_accuracy = 0
best_params = {}

print("\nğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì‹œì‘...\n")
results = []

for hidden_size, learning_rate in product(hidden_layer_sizes, learning_rates):
    print(f"\nğŸš€ í…ŒìŠ¤íŠ¸ ì¤‘ - ì€ë‹‰ì¸µ: {hidden_size}, í•™ìŠµë¥ : {learning_rate}")

    # âœ… ëª¨ë¸ ì´ˆê¸°í™”
    input_size = X.shape[1]
    #.shape[1]-ì—´ì˜ ê°œìˆ˜ (3ê°œ)
    output_size = num_classes
    #0,1,2,3 - 4ê°œ
  
    #ì´ˆê¸° ê°€ì¤‘ì¹˜ê°’
    W1 = np.random.randn(input_size + 1, hidden_size) * 0.01
    W2 = np.random.randn(hidden_size + 1, output_size) * 0.01

    mse_list = []
    acc_list = []

    for epoch in range(epochs):
        A2, Z1, A1, Z2, X_bias, H_bias = forward(X, W1, W2)
        W1, W2 = backward(X, X_bias, H_bias, y_one_hot, A2, Z1, A1, W1, W2, learning_rate)

        mse = np.mean((y_one_hot - A2) ** 2)
        mse_list.append(mse)

        predictions = np.argmax(A2, axis=1)#(1,4)
        accuracy = np.mean(predictions == y)
        acc_list.append(accuracy)

        if epoch % 500 == 0 or epoch == epochs - 1:
            print(f"ğŸ“¢ Epoch {epoch}/{epochs} - MSE: {mse:.4f}, Accuracy: {accuracy:.4f}")

    results.append({
        "hidden_size": hidden_size,
        "learning_rate": learning_rate,
        "epochs": epochs,
        "accuracy": accuracy
    })

    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_params = {
            "hidden_size": hidden_size,
            "learning_rate": learning_rate,
            "epochs": epochs,
            "accuracy": accuracy
        }
        # âœ… ëª¨ë¸ ì €ì¥ (ì •ê·œí™” ì •ë³´ë„ í•¨ê»˜ ì €ì¥)
        with open("best_ann_model.pkl", "wb") as f:
            pickle.dump((W1, W2, emotion_mapping, mean, scale), f)

print("\nğŸ¯ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì™„ë£Œ!\n")
print(f"âœ… ì€ë‹‰ì¸µ ê°œìˆ˜: {best_params['hidden_size']}")
print(f"âœ… í•™ìŠµë¥ : {best_params['learning_rate']}")
print(f"âœ… ì—í¬í¬: {best_params['epochs']}")
print(f"âœ… ìµœì¢… ì •í™•ë„: {best_params['accuracy']:.4f}")
print("ğŸ“¦ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: best_ann_model.pkl")
print("ğŸ“Š ë¼ë²¨ ë§¤í•‘:", emotion_mapping)


# âœ… ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
with open("best_hyperparameters.pkl", "wb") as f:
    pickle.dump(best_params, f)
